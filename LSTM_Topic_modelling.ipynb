{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Start by importing the necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from keras.models import Sequential"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "                               reviewId           userName  \\\n0  129510a0-b042-45f4-9f0a-3c497014652d         Jose Teles   \n1  1616dc35-a879-43c6-bc7b-183388e4cde2          Manchu_uk   \n2  d5f8ad28-78ae-446f-85d1-f6c1a2d1f0ef  Patient Onyealusi   \n3  e219a36b-cad0-4eed-bce3-6687587d0efc     mutungi marvin   \n4  ad2dd2fd-67fc-4423-bf94-15a2f98cff86       M Absar Asif   \n\n                                           userImage  \\\n0  https://play-lh.googleusercontent.com/a/AEdFTp...   \n1  https://play-lh.googleusercontent.com/a/AEdFTp...   \n2  https://play-lh.googleusercontent.com/a/AEdFTp...   \n3  https://play-lh.googleusercontent.com/a-/AD5-W...   \n4  https://play-lh.googleusercontent.com/a-/AD5-W...   \n\n                                             content  score  thumbsUpCount  \\\n0  The only problem is when you cash out ,cash ma...      3              0   \n1  It takes age's to receive card's, i have appli...      1              0   \n2                  I've not gotten my Account Number      3              0   \n3  I was denied an account with no explanation wh...      1              0   \n4  Not a good experience with the app. And also t...      1              0   \n\n  reviewCreatedVersion                   at  \\\n0                5.5.0  2022-12-23 21:09:04   \n1                5.4.0  2022-12-23 20:43:21   \n2                5.5.0  2022-12-23 17:10:06   \n3                5.5.0  2022-12-23 07:20:05   \n4                  NaN  2022-12-23 06:52:20   \n\n                                        replyContent            repliedAt  \n0  Hey Jose ðŸ‘‹ Thanks for taking the time to leave...  2022-12-24 10:04:55  \n1  Hi there ðŸ‘‹ Thanks for taking the time to post ...  2022-12-18 08:23:53  \n2  Hey there ðŸ‘‹ Thanks for your review! Sorry for ...  2022-12-24 09:48:35  \n3  Hi there ðŸ‘‹ We're sorry to hear that ðŸ˜” We'll as...  2022-12-23 07:24:34  \n4  Hi there ðŸ‘‹ Thanks for your feedback - we're so...  2022-12-23 06:57:57  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reviewId</th>\n      <th>userName</th>\n      <th>userImage</th>\n      <th>content</th>\n      <th>score</th>\n      <th>thumbsUpCount</th>\n      <th>reviewCreatedVersion</th>\n      <th>at</th>\n      <th>replyContent</th>\n      <th>repliedAt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>129510a0-b042-45f4-9f0a-3c497014652d</td>\n      <td>Jose Teles</td>\n      <td>https://play-lh.googleusercontent.com/a/AEdFTp...</td>\n      <td>The only problem is when you cash out ,cash ma...</td>\n      <td>3</td>\n      <td>0</td>\n      <td>5.5.0</td>\n      <td>2022-12-23 21:09:04</td>\n      <td>Hey Jose ðŸ‘‹ Thanks for taking the time to leave...</td>\n      <td>2022-12-24 10:04:55</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1616dc35-a879-43c6-bc7b-183388e4cde2</td>\n      <td>Manchu_uk</td>\n      <td>https://play-lh.googleusercontent.com/a/AEdFTp...</td>\n      <td>It takes age's to receive card's, i have appli...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5.4.0</td>\n      <td>2022-12-23 20:43:21</td>\n      <td>Hi there ðŸ‘‹ Thanks for taking the time to post ...</td>\n      <td>2022-12-18 08:23:53</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d5f8ad28-78ae-446f-85d1-f6c1a2d1f0ef</td>\n      <td>Patient Onyealusi</td>\n      <td>https://play-lh.googleusercontent.com/a/AEdFTp...</td>\n      <td>I've not gotten my Account Number</td>\n      <td>3</td>\n      <td>0</td>\n      <td>5.5.0</td>\n      <td>2022-12-23 17:10:06</td>\n      <td>Hey there ðŸ‘‹ Thanks for your review! Sorry for ...</td>\n      <td>2022-12-24 09:48:35</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>e219a36b-cad0-4eed-bce3-6687587d0efc</td>\n      <td>mutungi marvin</td>\n      <td>https://play-lh.googleusercontent.com/a-/AD5-W...</td>\n      <td>I was denied an account with no explanation wh...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5.5.0</td>\n      <td>2022-12-23 07:20:05</td>\n      <td>Hi there ðŸ‘‹ We're sorry to hear that ðŸ˜” We'll as...</td>\n      <td>2022-12-23 07:24:34</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ad2dd2fd-67fc-4423-bf94-15a2f98cff86</td>\n      <td>M Absar Asif</td>\n      <td>https://play-lh.googleusercontent.com/a-/AD5-W...</td>\n      <td>Not a good experience with the app. And also t...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>2022-12-23 06:52:20</td>\n      <td>Hi there ðŸ‘‹ Thanks for your feedback - we're so...</td>\n      <td>2022-12-23 06:57:57</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset of app reviews\n",
    "df = pd.read_csv('Monzo.csv')\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "(15031, 10)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for the shape of teh dataset\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "#Removing duplicates from the dataset\n",
    "df.drop_duplicates(\"content\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "(12864, 10)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for the shape of the dataset after removing duplicates\n",
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# Extract the text of the reviews and the corresponding ratings\n",
    "reviews = df['content'].values\n",
    "ratings = df['score'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['The only problem is when you cash out ,cash machine charge you a fee',\n       \"It takes age's to receive card's, i have applied 10 dats ago and they said i will receive on 2-3 days, I'm not going to use monzo even if the send me card Edit:- Its been 16 day's and not yet received, i applied for another bank like last week ago, they sent me everything and even internet banking is accessable\",\n       \"I've not gotten my Account Number\", ...,\n       'On the waiting list. Looking forward to all the new features to come.',\n       \"Mondo for Android - can't wait.  The iOS app is fantastic.  If Android is only half as good, would be great.\",\n       \"It is just the waiting list for now but it is a first step. I can't wait to get my mondo account!\"],\n      dtype=object)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing reviews\n",
    "reviews"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "array([3, 1, 3, ..., 5, 5, 5], dtype=int64)"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing ratings\n",
    "ratings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [45], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m v \u001B[38;5;241m=\u001B[39m TfidfVectorizer(decode_error\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreplace\u001B[39m\u001B[38;5;124m'\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 2\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcontent\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m x \u001B[38;5;241m=\u001B[39m v\u001B[38;5;241m.\u001B[39mfit_transform(df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mU\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[1;32m~\\Desktop\\School Work\\COM 726\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2121\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   2114\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[0;32m   2115\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[0;32m   2116\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[0;32m   2117\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[0;32m   2118\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[0;32m   2119\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[0;32m   2120\u001B[0m )\n\u001B[1;32m-> 2121\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[0;32m   2123\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[0;32m   2124\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\School Work\\COM 726\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[1;34m(self, raw_documents, y)\u001B[0m\n\u001B[0;32m   1369\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1370\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1371\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1372\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1373\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1374\u001B[0m             )\n\u001B[0;32m   1375\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m-> 1377\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1379\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[0;32m   1380\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\School Work\\COM 726\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1264\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[1;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[0;32m   1262\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[0;32m   1263\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m-> 1264\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m   1265\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1266\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[1;32m~\\Desktop\\School Work\\COM 726\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:106\u001B[0m, in \u001B[0;36m_analyze\u001B[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001B[39;00m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decoder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 106\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m analyzer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    108\u001B[0m     doc \u001B[38;5;241m=\u001B[39m analyzer(doc)\n",
      "File \u001B[1;32m~\\Desktop\\School Work\\COM 726\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:239\u001B[0m, in \u001B[0;36m_VectorizerMixin.decode\u001B[1;34m(self, doc)\u001B[0m\n\u001B[0;32m    236\u001B[0m     doc \u001B[38;5;241m=\u001B[39m doc\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode_error)\n\u001B[0;32m    238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m doc \u001B[38;5;129;01mis\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan:\n\u001B[1;32m--> 239\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    240\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    241\u001B[0m     )\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doc\n",
      "\u001B[1;31mValueError\u001B[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "v = TfidfVectorizer(decode_error='replace', encoding='utf-8')\n",
    "x = v.fit_transform(df['content'])\n",
    "x = v.fit_transform(df['content'].values.astype('U'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}